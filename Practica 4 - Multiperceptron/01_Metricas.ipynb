{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdWG2FEMDVST"
   },
   "source": [
    "# Métricas\n",
    "---\n",
    "**Precision, recall, F1 score y accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z0GYN5UFo29B"
   },
   "source": [
    "**Definición de funciones para métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 314,
     "status": "ok",
     "timestamp": 1695647942130,
     "user": {
      "displayName": "Cesar Estrebou",
      "userId": "12510511812709829812"
     },
     "user_tz": 180
    },
    "id": "44viJuybOEYD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd      # para trabajar con archivos de datos csv, excel, etc: https://pandas.pydata.org/docs/getting_started/tutorials.html\n",
    "import numpy as np\n",
    "\n",
    "# calcula las metricas precision, recall, f1-score y accuracy a partir de la matriz de confusion\n",
    "# retorna tupla: ( precision, recall, f1_score, accuracy )\n",
    "def calcular_metricas(conf_mat):\n",
    "    precision = np.zeros(conf_mat.shape[0])\n",
    "    for i in range(0, len(conf_mat)):\n",
    "        precision[i] = conf_mat[i][i]/sum(conf_mat.T[i])\n",
    "\n",
    "    recall = np.zeros(conf_mat.shape[0])\n",
    "    for i in range(0, len(conf_mat)):\n",
    "        recall[i] = conf_mat[i][i]/sum(conf_mat[i])\n",
    "\n",
    "    f1_score = 2* (precision*recall) /(precision+recall)\n",
    "\n",
    "    accuracy =  0\n",
    "    for i in range(0, len(conf_mat)):\n",
    "        accuracy+=conf_mat[i][i]\n",
    "    accuracy/= conf_mat.sum()\n",
    "\n",
    "    return ( precision, recall, f1_score, accuracy )\n",
    "\n",
    "# el parámetro metricas es una tupla ( precision, recall, f1_score, accuracy )\n",
    "def imprimir_metricas( metricas ):\n",
    "    (precision, recall, f1_score, accuracy) = metricas\n",
    "    print('\\n clase   precision    recall    f1-score')\n",
    "    for i in range(0, len(precision)):\n",
    "        print('%5d %10.2f %10.2f %10.2f' % (i, precision[i], recall[i], f1_score[i]))\n",
    "    print('\\naccuracy: %6.2f\\n' % accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8fH_UKQowDE"
   },
   "source": [
    "**Impresión de métricas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1695654199880,
     "user": {
      "displayName": "Cesar Estrebou",
      "userId": "12510511812709829812"
     },
     "user_tz": 180
    },
    "id": "QWZCkBM0o2OK",
    "outputId": "f0f84249-d58c-4a6e-c35c-8636ad46cf1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Matriz de Confusión:\n",
      "[[3 0 1]\n",
      " [2 2 0]\n",
      " [0 2 4]] \n",
      "\n",
      "\n",
      " clase   precision    recall    f1-score\n",
      "    0       0.60       0.75       0.67\n",
      "    1       0.50       0.50       0.50\n",
      "    2       0.80       0.67       0.73\n",
      "\n",
      "accuracy:   0.64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tienes tus etiquetas reales y predicciones para tres clases\n",
    "y_true = [0, 1, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 1, 2]  # Etiquetas reales\n",
    "y_pred = [0, 1, 2, 0, 1, 1, 2, 0, 2, 0, 1, 2, 0, 2]  # Predicciones del modelo\n",
    "\n",
    "confusion = confusion_matrix(y_true, y_pred)\n",
    "print('\\n Matriz de Confusión:')\n",
    "print(confusion, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "#calcula métricas de forma manual\n",
    "metricas = calcular_metricas(confusion)\n",
    "imprimir_metricas(metricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1695653582349,
     "user": {
      "displayName": "Cesar Estrebou",
      "userId": "12510511812709829812"
     },
     "user_tz": 180
    },
    "id": "Tw-x0w-hfhb8",
    "outputId": "235617da-53cf-4169-efe6-5a8f96292f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión (Accuracy): 0.64\n",
      "             Precisión por clase: [0.6 0.5 0.8]\n",
      "Exhaustividad (Recall) por clase: [0.75       0.5        0.66666667]\n",
      "         Puntuación F1 por clase: [0.66666667 0.5        0.72727273]\n",
      "Resultado de la clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.75      0.67         4\n",
      "           1       0.50      0.50      0.50         4\n",
      "           2       0.80      0.67      0.73         6\n",
      "\n",
      "    accuracy                           0.64        14\n",
      "   macro avg       0.63      0.64      0.63        14\n",
      "weighted avg       0.66      0.64      0.65        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn import metrics\n",
    "################################ calcula todas las métricas separadas ##############################\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print('Precisión (Accuracy): {:.2f}'.format(accuracy))\n",
    "\n",
    "precision = precision_score(y_true, y_pred, average=None) # precisión (precision) por clase\n",
    "print('             Precisión por clase:', precision)\n",
    "\n",
    "recall = recall_score(y_true, y_pred, average=None) # exhaustividad (recall) por clase\n",
    "print('Exhaustividad (Recall) por clase:', recall)\n",
    "\n",
    "# F1-score (otros nombre: F1-measure o F-beta score)\n",
    "f1 = f1_score(y_true, y_pred, average=None) # puntuación F1 por clase\n",
    "print('         Puntuación F1 por clase:', f1)\n",
    "\n",
    "################################ calcula todas las métricas juntas #################################\n",
    "report = metrics.classification_report(y_true, y_pred)\n",
    "print('Resultado de la clasificación:\\n%s' % report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO7iiKnbOCBwsTB6ZIJthR/",
   "provenance": [
    {
     "file_id": "1h9-ghQ8VXSPHLSG8Hkv8V3MqycXubDfN",
     "timestamp": 1667621061373
    },
    {
     "file_id": "1FV-Ydu5NBJ7DvGNfrlwMxfBHzXCqsO4A",
     "timestamp": 1667001788387
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
